{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "In the lecture we have implemented a few ML models, one using Tensorflow for predicting tomorrow's price the stock mtr (0066.HK).\n",
    "\n",
    "This question asks you to implement yet another ML model using any regression method\n",
    "available in the scikit-learn library, excluding Linear Regression and Neural Networks with\n",
    "the same set of stock price (i.e., 0066.HK between \"2010-\n",
    "01-01\" and \"2020-06-30\"). \n",
    "\n",
    "You should submit a Jupyter notebook that includes the\n",
    "three ML models (i.e., the Tensorflow implementation from the lectures and your\n",
    "implementations using sklearn), and compare their accuracy on predicting the\n",
    "price of 0066.HK during the period \"2021-01-01\" and \"2021-04-30\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reqiurements\n",
    "1. numpy\n",
    "2. pandas\n",
    "3. matplotlib\n",
    "4. scikit-learn\n",
    "5. tensorflow\n",
    "6. akshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%pip install akshare\n",
    "import akshare as ak # for getting stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock data\n",
    "symbol = \"00066\"\n",
    "start = \"2010-01-01\"\n",
    "end = \"2020-06-30\"\n",
    "predict_start = \"2021-01-01\"\n",
    "predict_end = \"2021-04-30\"\n",
    "stock_train = ak.stock_hk_hist(symbol=symbol, start_date=start, end_date=end, adjust='') # without adjust\n",
    "stock_predict_actual = ak.stock_hk_hist(symbol=symbol, start_date=predict_start, end_date=predict_end, adjust='') # without adjust\n",
    "\n",
    "# use GradientBoostingRegressor to predict stock price\n",
    "# stock_train columns: 日期/开盘/收盘/最高/最低/成交量/成交额/振幅/涨跌幅/涨跌额/换手率\n",
    "# use close price to predict\n",
    "\n",
    "# add lag and rolling features\n",
    "def create_features(data, lag_days=3, roll_days=3):\n",
    "    for lag in range(1, lag_days + 1):\n",
    "        data[f'lag_{lag}'] = data['收盘'].shift(lag)\n",
    "    data['rolling_mean'] = data['收盘'].rolling(window=roll_days).mean()\n",
    "    data['rolling_std'] = data['收盘'].rolling(window=roll_days).std()\n",
    "    data.dropna(inplace=True)  # drop rows with NaN values\n",
    "    return data\n",
    "\n",
    "# create features\n",
    "stock_train = create_features(stock_train)\n",
    "stock_predict_actual = create_features(stock_predict_actual)\n",
    "\n",
    "# form up X_train, y_train, X_test, y_test\n",
    "X_train = stock_train.drop('收盘', axis=1)\n",
    "y_train = stock_train['收盘']\n",
    "X_test = stock_predict_actual.drop('收盘', axis=1)\n",
    "y_test = stock_predict_actual['收盘']\n",
    "# select only numerical features\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "# define hyperparameters grid\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# use TimeSeriesSplit for cross-validation to avoid data leakage\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# fit the model and find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# predict using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Optimized MAE: {mae}\")\n",
    "print(f\"Optimized MSE: {mse}\")\n",
    "print(f\"Optimized RMSE: {rmse}\")\n",
    "\n",
    "# plot the predicted vs actual price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label=\"Actual Price\", color=\"blue\")\n",
    "plt.plot(y_test.index, y_pred, label=\"Predicted Price\", color=\"red\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Optimized Gradient Boosting Regressor - Predicted vs Actual Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Choose a suitable method (except neural network) from sklearn to train a\n",
    "Machine Learning model using the MNIST data set for hand-written digit classification.\n",
    "Provide a brief explanation of your chosen method and why it is suitable for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of SVM\n",
    "SVM is a supervised machine learning algorithm used for both classification and regression tasks. In classification, SVM works by finding an optimal hyperplane that best separates the data points of different classes. For linearly separable data, this hyperplane maximizes the margin between the two classes. For data that is not linearly separable, SVM uses a kernel trick to map data into a higher-dimensional space, where it becomes easier to classify with a hyperplane.\n",
    "\n",
    "## Key Reasons Why SVM is Suitable for MNIST\n",
    "1. Effective in High-Dimensional Spaces: MNIST consists of 28x28 pixel images, which translates to a 784-dimensional feature space when each pixel is treated as an individual feature. SVM is known for its effectiveness in handling high-dimensional data.\n",
    "\n",
    "2. Well-Suited for Small to Medium-Size Datasets: While neural networks excel on very large datasets, SVM can perform comparably well on smaller datasets and doesn't require as extensive tuning.\n",
    "\n",
    "3. Ability to Handle Non-Linear Boundaries: With the use of non-linear kernels (e.g., the RBF kernel), SVM can efficiently handle the curved decision boundaries present in complex datasets like MNIST.\n",
    "\n",
    "4. Robustness to Overfitting: By maximizing the margin between classes, SVM tends to generalize well, reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the SVM model with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "\n",
    "# Train the SVM model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize some predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Prediction: {y_pred[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
